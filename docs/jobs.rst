.. _jobs:

####
Jobs
####

Input Parameters
================
Users can define these parameters in the workflow specification file or through CLI/API input
commands:

- **command**: String invoked by torc to run a job. It is typically an executable with a unique set of
  arguments and options. The executable can look up other input parameters from the database.
  Refer to :ref:`job_input_output_data` for a discussion of of how to store input and output data
  for jobs.
- **cancel_on_blocking_job_failure**: If this is set to true and a job upon which this job is dependent
  fails, torc will cancel this job.
- **supports_termination**: Should be set to true if the job handles the signal ``SIGTERM``. Refer to
  :ref:`job_graceful_shutdown`.

Torc Parameters
===============
- **key**: Unique identifier of the job in the database. By default, generated by the database. Users
  can define their own keys, but this is not recommended in most situations.
- **status**: Current status of the job in a workflow. Refer to :ref:`job_status`.
  workflow is restarted.
- **run_id**: Integer representing one job execution. Starts at one and increases every time the

.. _job_resource_requirements:

Resource Requirements
=====================
You can store definitions of job resource requirements in the database and then associate them with
jobs. This is critical because it informs torc about what jobs can be run in parallel on a single
compute node.

The recommended way of defining these relationships is through the workflow specification (JSON5)
file. One set of resource requirements looks like this:

.. code-block:: JavaScript

    {
      name: "large",
      num_cpus: 36,
      num_gpus: 0,
      num_nodes: 1,
      memory: "80g",
      runtime: "P0DT12H"
    }

This says that any job assigned these requirements will consume 36 CPUs, 80 GB of memory, and run
for 12 hours.

You assign one or more jobs to these requirements in the ``resource_requirements`` field of the job
specification: ``resource_requirements: "large"``.

.. _job_input_output_data:

Job Input/Output Data
=====================

Torc provides a mechanism for users to store input and output data in the database. This data can
be stored on a per-job basis or for the overall workflow.

One way to run jobs with different parameters is to pass those parameters as command-line arguments
and options. A second way is to store the input parameters in the ``user_data`` collection of the
database. A common runner script can pull the parameters for each specific job at runtime.

.. note:: Torc sets the environment variable TORC_JOB_KEY with each job's unique key. Scripts can
   use this value to retrieve data from the database.

Jobs can also store result data and metatdata in the database.

.. warning:: The database is not currently designed to store large result data. You can store
   small result data or pointers to where the actual data resides.

Here is how to store and retrieve user data from torc CLI commands and API commands.

These examples add two JSON objects to the job.

Torc CLI
--------

.. code-block:: console

   $ torc jobs add-user-data 92181820 "{key1: 'val1', key2: 'val2'}" "{key3: 'val3'}"
   2023-03-29 08:21:33,553 - INFO [torc.cli.jobs jobs.py:103] : Added user_data key=92340362 to job key=92181820
   2023-03-29 08:21:33,613 - INFO [torc.cli.jobs jobs.py:103] : Added user_data key=92340378 to job key=92181820


.. code-block:: console

   $ torc jobs list-user-data 92181820
   [
     {
       "_key": "92340362",
       "_rev": "_fw4IkZ----",
       "key3": "val3"
     },
     {
       "_key": "92340378",
       "_rev": "_fw4IkX----",
       "key1": "val1",
       "key2": "val2"
     }
   ]


.. code-block:: console

   $ torc user-data add "{key1: 'val1', key2: 'val2'}" "{key3: 'val3'}"
   2023-03-29 09:45:59,678 - INFO [torc.cli.user_data user_data.py:41] : Added user_data key=92398595
   2023-03-29 09:45:59,736 - INFO [torc.cli.user_data user_data.py:41] : Added user_data key=92398602

   $ torc user-data list
   [
     {
       "_key": "92398595",
       "_rev": "_fw4IkX----",
       "key1": "val1",
       "key2": "val2"
     },
     {
       "_key": "92398602",
       "_rev": "_fw4IkZ----",
       "key3": "val3"
     }
   ]

   $ torc user-data get 92398595
   {
     '_key': '92398595',
     '_rev': '_fw2IcgK---',
     'key1': 'val1',
     'key2': 'val2'
   }

   $ torc user-data delete 92398595 92398602
   2023-03-29 09:47:56,772 - INFO [torc.cli.user_data user_data.py:54] : Deleted user_data=92398595
   2023-03-29 09:47:56,799 - INFO [torc.cli.user_data user_data.py:54] : Deleted user_data=92398602


Python API client
-----------------

.. code-block:: python

    from swagger_client import ApiClient, DefaultApi
    from swagger_client.configuration import Configuration

    configuration = Configuration()
    configuration.host = "http://localhost:8529/_db/workflows/torc-service"
    api = DefaultApi(ApiClient(configuration))
    workflow_key = "92400133"
    job_key = "92400255"
    data = [
        {
            "key1": "val1",
            "key2": "val2",
        },
        {
            "key3": "val3",
        },
    ]
    for item in data:
        result = api.post_workflows_workflow_jobs_user_data_key(item, workflow_key, job_key)
        print(f"Added user data key={result['_key']}")

    result = api.get_workflows_workflow_jobs_user_data_key(workflow_key, job_key)
    print(f"Job key={job_key} stores {result.items}")

    workflow_user_data = api.post_workflows_workflow_user_data(data[0], workflow_key)
    result = api.get_workflows_workflow_user_data_key(workflow_key, workflow_user_data["_key"])
    print(f"Workflow stores user data {result}")

.. _job_status:

Job Statuses
============
- **uninitialized**: Initial state. Not yet known if it is blocked or ready.
- **ready**: The job can be submitted.
- **blocked**: The job cannot start because of dependencies.
- **submitted_pending**: The job was given to a compute node but is not yet running.
- **submitted**: The job is running on a compute node.
- **terminated**: Compute node timeout occurred and the job was notified to checkpoint and shut
  down.
- **done**: The job finished. It may or may not have completed successfully.
- **canceled**: A blocking job failed and so the job never ran.
- **disabled**: The job cannot run or change state.

.. graphviz::

   digraph job_statuses {
      "uninitialized" -> "ready";
      "uninitialized" -> "blocked";
      "uninitialized" -> "disabled";
      "disabled" -> "uninitialized";
      "ready" -> "submitted_pending";
      "submitted_pending" -> "submitted";
      "submitted" -> "done";
      "submitted" -> "terminated";
      "blocked" -> "canceled";
      "blocked" -> "ready";
   }

.. raw:: html

   <hr>

.. _job_graceful_shutdown:

Graceful shutdown of jobs
=========================
A common error condition in HPC environments is underestimating the walltime for a job. The HPC
scheduler will kill the job. If you don't take precautions, you will lose the work and have to
start from the beginning.

Similar to Slurm, Torc offers one procedure to help with this problem: the
``supports_termination`` flag in the job defintion. If this is set to true then torc will send the
signal ``SIGTERM`` to each job process. If your job registers a signal handler for that signal, you
can gracefully shutdown such that a subsequent process can resume where it left off.

Don't set this flag if your job doesn't catch SIGTERM. Torc will attempt to wait for the process
exit and capture its return code.

Torc performs these actions two minutes before the walltime timeout. (This could be made
customizable.)

Refer to this script for a Python example of detecting this signal:
https://github.nrel.gov/viz/wms/blob/main/torc/tests/scripts/sleep.py

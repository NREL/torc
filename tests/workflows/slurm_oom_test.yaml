# Slurm OOM Test Workflow
#
# This workflow is designed to test Slurm debugging features:
# - torc slurm parse-logs: Should detect OOM errors in Slurm logs
# - torc slurm sacct: Should show OUT_OF_MEMORY state
# - torc-dash Debugging tab: Should display the errors
#
# The job requests 200GB memory but tries to allocate more,
# triggering Slurm's OOM killer.
#
# Usage:
#   1. Create workflow: torc workflows create tests/workflows/slurm_oom_test.yaml
#   2. Submit to Slurm: torc slurm schedule-nodes <workflow_id>
#   3. Wait for job to fail with OOM
#   4. Test debugging tools:
#      - torc slurm parse-logs <workflow_id>
#      - torc slurm sacct <workflow_id>
#      - Open torc-dash and check Debugging tab

name: slurm_oom_test

# Enable resource monitoring to capture memory usage over time
# This helps visualize how memory grows before OOM occurs
resource_monitor:
  enabled: true
  granularity: "time_series"
  sample_interval_seconds: 1

jobs:
  - name: oom_job
    command: bash tests/scripts/oom_test.sh 20 10
    resource_requirements: high_memory
    scheduler: oom_test_scheduler

# Resource requirements - job requests 200GB memory
resource_requirements:
  - name: high_memory
    num_cpus: 10
    num_gpus: 0
    num_nodes: 1
    memory: 200g
    runtime: PT30M  # 30 minutes should be enough

# Slurm scheduler configuration - allocates 240GB to allow for overhead
slurm_schedulers:
  - name: oom_test_scheduler
    account: myaccount
    mem: 240G
    nodes: 1
    walltime: "01:00:00"

# Automatically schedule Slurm nodes when workflow starts
actions:
  - trigger_type: "on_workflow_start"
    action_type: "schedule_nodes"
    scheduler: "oom_test_scheduler"
    scheduler_type: "slurm"
    num_allocations: 1

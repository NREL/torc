name: "slurm_staged_pipeline"
description: "Multi-stage Slurm pipeline with automated scheduling and resource monitoring"
user: "pipeline_user"

# Define three Slurm schedulers for different pipeline stages
slurm_schedulers:
  - name: "setup_scheduler"
    account: "my_account"
    walltime: "04:00:00"
    nodes: 1
  - name: "work_scheduler"
    account: "my_account"
    walltime: "04:00:00"
    nodes: 1
  - name: "postprocess_scheduler"
    account: "my_account"
    walltime: "04:00:00"
    nodes: 1

# Define resource requirements for each stage
resource_requirements:
  - name: "setup_resources"
    num_cpus: 16
    num_gpus: 0
    memory: "64g"
    runtime: "PT4H"
    num_nodes: 1

  - name: "work_resources"
    num_cpus: 32
    num_gpus: 0
    memory: "300g"
    runtime: "PT8H"
    num_nodes: 1

  - name: "postprocess_resources"
    num_cpus: 8
    num_gpus: 0
    memory: "32g"
    runtime: "PT1H"
    num_nodes: 1

# Enable time-series resource monitoring
resource_monitor:
  enabled: true
  sample_interval_seconds: 5
  granularity: "time_series"
  generate_plots: false
  metrics:
    - cpu_percent
    - memory_bytes

# Define scheduling rules for automated stage execution
# via the compute node allocation API before starting the workflow
scheduling_rules:
  # Stage 1: Launch setup job when workflow starts
  - trigger_type: "on_workflow_start"
    start_one_worker_per_node: false

  # Stage 2: Launch work jobs when setup completes (all jobs ready)
  - trigger_type: "when_jobs_ready"
    start_one_worker_per_node: true

  # Stage 3: Launch postprocessing when all work jobs complete
  - trigger_type: "when_jobs_complete"
    start_one_worker_per_node: false

# Define input files created by setup job
files:
  - name: "input_001"
    path: "/scratch/pipeline/input_001.dat"
  - name: "input_002"
    path: "/scratch/pipeline/input_002.dat"
  - name: "input_003"
    path: "/scratch/pipeline/input_003.dat"
  - name: "input_004"
    path: "/scratch/pipeline/input_004.dat"
  - name: "input_005"
    path: "/scratch/pipeline/input_005.dat"
  - name: "input_006"
    path: "/scratch/pipeline/input_006.dat"
  - name: "input_007"
    path: "/scratch/pipeline/input_007.dat"
  - name: "input_008"
    path: "/scratch/pipeline/input_008.dat"
  - name: "input_009"
    path: "/scratch/pipeline/input_009.dat"
  - name: "input_010"
    path: "/scratch/pipeline/input_010.dat"

  # Output files created by work jobs
  - name: "output_001"
    path: "/scratch/pipeline/output_001.dat"
  - name: "output_002"
    path: "/scratch/pipeline/output_002.dat"
  - name: "output_003"
    path: "/scratch/pipeline/output_003.dat"
  - name: "output_004"
    path: "/scratch/pipeline/output_004.dat"
  - name: "output_005"
    path: "/scratch/pipeline/output_005.dat"
  - name: "output_006"
    path: "/scratch/pipeline/output_006.dat"
  - name: "output_007"
    path: "/scratch/pipeline/output_007.dat"
  - name: "output_008"
    path: "/scratch/pipeline/output_008.dat"
  - name: "output_009"
    path: "/scratch/pipeline/output_009.dat"
  - name: "output_010"
    path: "/scratch/pipeline/output_010.dat"

  # Final results file
  - name: "final_results"
    path: "/scratch/pipeline/final_results.dat"

# Define the three-stage pipeline
jobs:
  # Stage 1: Setup job - generates 10 input files
  - name: "setup"
    command: |
      #!/bin/bash
      set -e
      echo "Starting setup stage at $(date)"

      # Create scratch directory
      mkdir -p /scratch/pipeline

      # Generate 10 input data files
      for i in $(seq -f "%03g" 1 10); do
        echo "Generating input_${i}.dat..."
        # Simulate data generation with random data
        dd if=/dev/urandom of=/scratch/pipeline/input_${i}.dat bs=1M count=1024 2>/dev/null
        echo "Created input file $i with $(stat -f%z /scratch/pipeline/input_${i}.dat) bytes"
      done

      echo "Setup stage completed at $(date)"
      echo "Generated 10 input files in /scratch/pipeline/"
    scheduler_name: "setup_scheduler"
    resource_requirements_name: "setup_resources"
    output_file_names:
      - "input_001"
      - "input_002"
      - "input_003"
      - "input_004"
      - "input_005"
      - "input_006"
      - "input_007"
      - "input_008"
      - "input_009"
      - "input_010"

  # Stage 2: Work jobs - process input files in parallel
  - name: "work_001"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_001 at $(date)"
      echo "Processing /scratch/pipeline/input_001.dat"

      # Simulate compute-intensive processing
      # Allocate memory to demonstrate 300GB requirement
      python3 -c "
      import numpy as np
      import time
      print('Allocating large arrays for processing...')
      # Simulate heavy computation with large memory footprint
      data = np.random.rand(1000000, 100)
      result = np.fft.fft2(data)
      time.sleep(60)  # Simulate long computation
      print('Processing complete')
      "

      # Generate output file
      dd if=/dev/urandom of=/scratch/pipeline/output_001.dat bs=1M count=512 2>/dev/null
      echo "Work_001 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_001"]
    output_file_names: ["output_001"]

  - name: "work_002"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_002 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_002.dat bs=1M count=512 2>/dev/null
      echo "Work_002 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_002"]
    output_file_names: ["output_002"]

  - name: "work_003"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_003 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_003.dat bs=1M count=512 2>/dev/null
      echo "Work_003 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_003"]
    output_file_names: ["output_003"]

  - name: "work_004"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_004 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_004.dat bs=1M count=512 2>/dev/null
      echo "Work_004 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_004"]
    output_file_names: ["output_004"]

  - name: "work_005"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_005 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_005.dat bs=1M count=512 2>/dev/null
      echo "Work_005 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_005"]
    output_file_names: ["output_005"]

  - name: "work_006"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_006 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_006.dat bs=1M count=512 2>/dev/null
      echo "Work_006 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_006"]
    output_file_names: ["output_006"]

  - name: "work_007"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_007 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_007.dat bs=1M count=512 2>/dev/null
      echo "Work_007 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_007"]
    output_file_names: ["output_007"]

  - name: "work_008"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_008 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_008.dat bs=1M count=512 2>/dev/null
      echo "Work_008 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_008"]
    output_file_names: ["output_008"]

  - name: "work_009"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_009 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_009.dat bs=1M count=512 2>/dev/null
      echo "Work_009 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_009"]
    output_file_names: ["output_009"]

  - name: "work_010"
    command: |
      #!/bin/bash
      set -e
      echo "Starting work_010 at $(date)"
      python3 -c "import numpy as np; import time; data = np.random.rand(1000000, 100); result = np.fft.fft2(data); time.sleep(60)"
      dd if=/dev/urandom of=/scratch/pipeline/output_010.dat bs=1M count=512 2>/dev/null
      echo "Work_010 completed at $(date)"
    scheduler_name: "work_scheduler"
    resource_requirements_name: "work_resources"
    blocked_by_job_names: ["setup"]
    input_file_names: ["input_010"]
    output_file_names: ["output_010"]

  # Stage 3: Post-processing job - aggregates all work outputs
  - name: "postprocess"
    command: |
      #!/bin/bash
      set -e
      echo "Starting postprocessing at $(date)"

      # Verify all input files exist
      for i in $(seq -f "%03g" 1 10); do
        if [ ! -f "/scratch/pipeline/output_${i}.dat" ]; then
          echo "ERROR: Missing output file output_${i}.dat"
          exit 1
        fi
        echo "Found output_${i}.dat: $(stat -f%z /scratch/pipeline/output_${i}.dat) bytes"
      done

      # Aggregate results
      echo "Aggregating results from 10 work jobs..."
      cat /scratch/pipeline/output_*.dat > /scratch/pipeline/final_results.dat

      # Generate summary statistics
      total_size=$(stat -f%z /scratch/pipeline/final_results.dat)
      num_files=$(ls -1 /scratch/pipeline/output_*.dat | wc -l)

      echo "Pipeline Summary:" | tee -a /scratch/pipeline/final_results.dat
      echo "  Total output files processed: $num_files" | tee -a /scratch/pipeline/final_results.dat
      echo "  Final results size: $total_size bytes" | tee -a /scratch/pipeline/final_results.dat
      echo "  Completed at: $(date)" | tee -a /scratch/pipeline/final_results.dat

      echo "Postprocessing completed successfully"
    scheduler_name: "postprocess_scheduler"
    resource_requirements_name: "postprocess_resources"
    blocked_by_job_names:
      - "work_001"
      - "work_002"
      - "work_003"
      - "work_004"
      - "work_005"
      - "work_006"
      - "work_007"
      - "work_008"
      - "work_009"
      - "work_010"
    input_file_names:
      - "output_001"
      - "output_002"
      - "output_003"
      - "output_004"
      - "output_005"
      - "output_006"
      - "output_007"
      - "output_008"
      - "output_009"
      - "output_010"
    output_file_names:
      - "final_results"

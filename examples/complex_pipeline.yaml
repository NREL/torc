# Complex Scientific Data Processing Pipeline
# This workflow demonstrates a multi-stage data processing pipeline with 5 stages:
# Stage 1: Data Ingestion (4 jobs) - Download and preprocess raw datasets
# Stage 2: Data Cleaning (4 jobs) - Clean and validate the preprocessed data
# Stage 3: Feature Engineering (4 jobs) - Extract features from cleaned data
# Stage 4: Analysis (4 jobs) - Perform statistical analysis and modeling
# Stage 5: Reporting (4 jobs) - Generate reports and visualizations

name: complex_scientific_pipeline
user: scientist
description: A complex 5-stage scientific data processing pipeline with file dependencies

# ===== STAGE 1: DATA INGESTION =====
jobs:
  # Download raw datasets from different sources
  - name: download_sensor_data
    command: |
      wget https://example.com/sensors/temperature_data.csv -O raw_sensor_data.csv &&
      echo "Downloaded $(wc -l < raw_sensor_data.csv) sensor readings"
    invocation_script: |
      #!/bin/bash
      set -e
      mkdir -p /data/raw
    cancel_on_blocking_job_failure: false
    supports_termination: true
    resource_requirements_name: download_job
    blocked_by_job_names: null
    input_file_names: null
    output_file_names:
      - raw_sensor_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: io_scheduler

  - name: download_weather_data
    command: |
      curl -o raw_weather_data.json "https://api.weather.com/v1/historical?date=2024-01-01" &&
      echo "Weather data download completed"
    invocation_script: |
      #!/bin/bash
      set -e
      mkdir -p /data/raw
    cancel_on_blocking_job_failure: false
    supports_termination: true
    resource_requirements_name: download_job
    blocked_by_job_names: null
    input_file_names: null
    output_file_names:
      - raw_weather_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: io_scheduler

  - name: download_satellite_data
    command: |
      python download_satellite.py --region="north_america" --output=raw_satellite_data.tiff &&
      echo "Satellite imagery downloaded successfully"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate geo_env
      mkdir -p /data/raw
    cancel_on_blocking_job_failure: false
    supports_termination: true
    resource_requirements_name: download_job
    blocked_by_job_names: null
    input_file_names:
      - satellite_script
    output_file_names:
      - raw_satellite_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: io_scheduler

  - name: preprocess_datasets
    command: |
      python preprocess.py --sensor=raw_sensor_data.csv --weather=raw_weather_data.json --satellite=raw_satellite_data.tiff &&
      echo "Preprocessing completed for all datasets"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate data_env
      export PYTHONPATH=/opt/preprocessing:$PYTHONPATH
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - download_sensor_data
      - download_weather_data
      - download_satellite_data
    input_file_names:
      - raw_sensor_data
      - raw_weather_data
      - raw_satellite_data
      - preprocessing_script
    output_file_names:
      - preprocessed_combined_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  # ===== STAGE 2: DATA CLEANING =====
  - name: validate_data_quality
    command: |
      python validate_quality.py --input=preprocessed_combined_data.parquet --output=validated_data.parquet &&
      echo "Data quality validation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate data_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - preprocess_datasets
    input_file_names:
      - preprocessed_combined_data
      - validation_script
    output_file_names:
      - validated_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: handle_missing_values
    command: |
      python impute_missing.py --input=validated_data.parquet --method=knn --output=imputed_data.parquet &&
      echo "Missing value imputation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
    cancel_on_blocking_job_failure: true
    supports_termination: false
    resource_requirements_name: memory_intensive_job
    blocked_by_job_names:
      - validate_data_quality
    input_file_names:
      - validated_data
      - imputation_script
    output_file_names:
      - imputed_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: memory_scheduler

  - name: detect_outliers
    command: |
      python detect_outliers.py --input=imputed_data.parquet --method=isolation_forest --output=outlier_analysis.json &&
      echo "Outlier detection completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - handle_missing_values
    input_file_names:
      - imputed_data
      - outlier_detection_script
    output_file_names:
      - outlier_analysis
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: clean_final_dataset
    command: |
      python clean_data.py --input=imputed_data.parquet --outliers=outlier_analysis.json --output=clean_data.parquet &&
      echo "Final dataset cleaning completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate data_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - handle_missing_values
      - detect_outliers
    input_file_names:
      - imputed_data
      - outlier_analysis
      - cleaning_script
    output_file_names:
      - clean_data
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  # ===== STAGE 3: FEATURE ENGINEERING =====
  - name: extract_temporal_features
    command: |
      python extract_temporal.py --input=clean_data.parquet --output=temporal_features.parquet &&
      echo "Temporal feature extraction completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - clean_final_dataset
    input_file_names:
      - clean_data
      - temporal_feature_script
    output_file_names:
      - temporal_features
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: extract_spatial_features
    command: |
      python extract_spatial.py --input=clean_data.parquet --output=spatial_features.parquet &&
      echo "Spatial feature extraction completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate geo_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - clean_final_dataset
    input_file_names:
      - clean_data
      - spatial_feature_script
    output_file_names:
      - spatial_features
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: compute_statistical_features
    command: |
      python compute_stats.py --input=clean_data.parquet --window=7d --output=statistical_features.parquet &&
      echo "Statistical feature computation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate stats_env
    cancel_on_blocking_job_failure: true
    supports_termination: false
    resource_requirements_name: memory_intensive_job
    blocked_by_job_names:
      - clean_final_dataset
    input_file_names:
      - clean_data
      - statistical_script
    output_file_names:
      - statistical_features
    input_user_data_names: null
    output_data_names: null
    scheduler_name: memory_scheduler

  - name: combine_features
    command: |
      python combine_features.py --temporal=temporal_features.parquet --spatial=spatial_features.parquet --stats=statistical_features.parquet --output=combined_features.parquet &&
      echo "Feature combination completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: memory_intensive_job
    blocked_by_job_names:
      - extract_temporal_features
      - extract_spatial_features
      - compute_statistical_features
    input_file_names:
      - temporal_features
      - spatial_features
      - statistical_features
      - feature_combination_script
    output_file_names:
      - combined_features
    input_user_data_names: null
    output_data_names: null
    scheduler_name: memory_scheduler

  # ===== STAGE 4: ANALYSIS =====
  - name: exploratory_data_analysis
    command: |
      python eda.py --input=combined_features.parquet --output=eda_results.html &&
      echo "Exploratory data analysis completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate analysis_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - combine_features
    input_file_names:
      - combined_features
      - eda_script
    output_file_names:
      - eda_results
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: correlation_analysis
    command: |
      python correlation_analysis.py --input=combined_features.parquet --output=correlation_matrix.png &&
      echo "Correlation analysis completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate analysis_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - combine_features
    input_file_names:
      - combined_features
      - correlation_script
    output_file_names:
      - correlation_matrix
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: train_predictive_model
    command: |
      python train_model.py --input=combined_features.parquet --model=random_forest --output=trained_model.pkl &&
      echo "Predictive model training completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
      export CUDA_VISIBLE_DEVICES=0
    cancel_on_blocking_job_failure: true
    supports_termination: false
    resource_requirements_name: gpu_job
    blocked_by_job_names:
      - combine_features
    input_file_names:
      - combined_features
      - model_training_script
    output_file_names:
      - trained_model
    input_user_data_names: null
    output_data_names: null
    scheduler_name: gpu_scheduler

  - name: model_evaluation
    command: |
      python evaluate_model.py --model=trained_model.pkl --test_data=combined_features.parquet --output=evaluation_results.json &&
      echo "Model evaluation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate ml_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - train_predictive_model
    input_file_names:
      - trained_model
      - combined_features
      - evaluation_script
    output_file_names:
      - evaluation_results
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  # ===== STAGE 5: REPORTING =====
  - name: generate_summary_report
    command: |
      python generate_report.py --eda=eda_results.html --evaluation=evaluation_results.json --output=summary_report.pdf &&
      echo "Summary report generation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate reporting_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - exploratory_data_analysis
      - model_evaluation
    input_file_names:
      - eda_results
      - evaluation_results
      - report_template
    output_file_names:
      - summary_report
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: create_visualizations
    command: |
      python create_viz.py --features=combined_features.parquet --correlations=correlation_matrix.png --output=visualizations.html &&
      echo "Visualization creation completed"
    invocation_script: |
      #!/bin/bash
      set -e
      source /opt/conda/bin/activate viz_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: compute_job
    blocked_by_job_names:
      - combine_features
      - correlation_analysis
    input_file_names:
      - combined_features
      - correlation_matrix
      - visualization_script
    output_file_names:
      - interactive_visualizations
    input_user_data_names: null
    output_data_names: null
    scheduler_name: compute_scheduler

  - name: export_results
    command: |
      python export_results.py --model=trained_model.pkl --features=combined_features.parquet --report=summary_report.pdf --viz=visualizations.html --output=/results/final_output &&
      echo "Results export completed"
    invocation_script: |
      #!/bin/bash
      set -e
      mkdir -p /results/final_output
      source /opt/conda/bin/activate export_env
    cancel_on_blocking_job_failure: true
    supports_termination: true
    resource_requirements_name: io_job
    blocked_by_job_names:
      - train_predictive_model
      - generate_summary_report
      - create_visualizations
    input_file_names:
      - trained_model
      - combined_features
      - summary_report
      - interactive_visualizations
      - export_script
    output_file_names:
      - final_results_archive
    input_user_data_names: null
    output_data_names: null
    scheduler_name: io_scheduler

  - name: cleanup_intermediate_files
    command: |
      python cleanup.py --workflow_id=$TORC_WORKFLOW_ID --keep_final=true &&
      echo "Cleanup of intermediate files completed"
    invocation_script: |
      #!/bin/bash
      set -e
      # Clean up large intermediate files to save disk space
    cancel_on_blocking_job_failure: false
    supports_termination: true
    resource_requirements_name: minimal_job
    blocked_by_job_names:
      - export_results
    input_file_names:
      - cleanup_script
    output_file_names: null
    input_user_data_names: null
    output_data_names: null
    scheduler_name: io_scheduler

# ===== FILE DEFINITIONS =====
files:
  # Stage 1: Raw data files
  - name: raw_sensor_data
    path: /data/raw/sensor_data.csv
    st_mtime: null

  - name: raw_weather_data
    path: /data/raw/weather_data.json
    st_mtime: null

  - name: raw_satellite_data
    path: /data/raw/satellite_data.tiff
    st_mtime: null

  - name: preprocessed_combined_data
    path: /data/processed/combined_data.parquet
    st_mtime: null

  # Stage 2: Cleaned data files
  - name: validated_data
    path: /data/cleaned/validated_data.parquet
    st_mtime: null

  - name: imputed_data
    path: /data/cleaned/imputed_data.parquet
    st_mtime: null

  - name: outlier_analysis
    path: /data/analysis/outlier_analysis.json
    st_mtime: null

  - name: clean_data
    path: /data/cleaned/clean_data.parquet
    st_mtime: null

  # Stage 3: Feature files
  - name: temporal_features
    path: /data/features/temporal_features.parquet
    st_mtime: null

  - name: spatial_features
    path: /data/features/spatial_features.parquet
    st_mtime: null

  - name: statistical_features
    path: /data/features/statistical_features.parquet
    st_mtime: null

  - name: combined_features
    path: /data/features/combined_features.parquet
    st_mtime: null

  # Stage 4: Analysis results
  - name: eda_results
    path: /data/analysis/eda_results.html
    st_mtime: null

  - name: correlation_matrix
    path: /data/analysis/correlation_matrix.png
    st_mtime: null

  - name: trained_model
    path: /data/models/trained_model.pkl
    st_mtime: null

  - name: evaluation_results
    path: /data/analysis/evaluation_results.json
    st_mtime: null

  # Stage 5: Final outputs
  - name: summary_report
    path: /results/summary_report.pdf
    st_mtime: null

  - name: interactive_visualizations
    path: /results/visualizations.html
    st_mtime: null

  - name: final_results_archive
    path: /results/final_output/results.tar.gz
    st_mtime: null

  # Script files (pre-existing)
  - name: satellite_script
    path: /scripts/download_satellite.py
    st_mtime: 1704067200.0

  - name: preprocessing_script
    path: /scripts/preprocess.py
    st_mtime: 1704067200.0

  - name: validation_script
    path: /scripts/validate_quality.py
    st_mtime: 1704067200.0

  - name: imputation_script
    path: /scripts/impute_missing.py
    st_mtime: 1704067200.0

  - name: outlier_detection_script
    path: /scripts/detect_outliers.py
    st_mtime: 1704067200.0

  - name: cleaning_script
    path: /scripts/clean_data.py
    st_mtime: 1704067200.0

  - name: temporal_feature_script
    path: /scripts/extract_temporal.py
    st_mtime: 1704067200.0

  - name: spatial_feature_script
    path: /scripts/extract_spatial.py
    st_mtime: 1704067200.0

  - name: statistical_script
    path: /scripts/compute_stats.py
    st_mtime: 1704067200.0

  - name: feature_combination_script
    path: /scripts/combine_features.py
    st_mtime: 1704067200.0

  - name: eda_script
    path: /scripts/eda.py
    st_mtime: 1704067200.0

  - name: correlation_script
    path: /scripts/correlation_analysis.py
    st_mtime: 1704067200.0

  - name: model_training_script
    path: /scripts/train_model.py
    st_mtime: 1704067200.0

  - name: evaluation_script
    path: /scripts/evaluate_model.py
    st_mtime: 1704067200.0

  - name: report_template
    path: /templates/report_template.html
    st_mtime: 1704067200.0

  - name: visualization_script
    path: /scripts/create_viz.py
    st_mtime: 1704067200.0

  - name: export_script
    path: /scripts/export_results.py
    st_mtime: 1704067200.0

  - name: cleanup_script
    path: /scripts/cleanup.py
    st_mtime: 1704067200.0



# ===== RESOURCE REQUIREMENTS =====
resource_requirements:
  - name: minimal_job
    num_cpus: 1
    num_gpus: 0
    num_nodes: 1
    memory: 512m
    runtime: PT5M

  - name: download_job
    num_cpus: 2
    num_gpus: 0
    num_nodes: 1
    memory: 1g
    runtime: PT15M

  - name: io_job
    num_cpus: 2
    num_gpus: 0
    num_nodes: 1
    memory: 2g
    runtime: PT10M

  - name: compute_job
    num_cpus: 4
    num_gpus: 0
    num_nodes: 1
    memory: 8g
    runtime: PT30M

  - name: memory_intensive_job
    num_cpus: 8
    num_gpus: 0
    num_nodes: 1
    memory: 32g
    runtime: PT1H

  - name: gpu_job
    num_cpus: 8
    num_gpus: 1
    num_nodes: 1
    memory: 16g
    runtime: PT2H

# ===== SLURM SCHEDULER CONFIGURATIONS =====
slurm_schedulers:
  - name: io_scheduler
    account: data_project
    gres: null
    mem: 4G
    nodes: 1
    ntasks_per_node: 1
    partition: io
    qos: normal
    tmp: 20G
    walltime: "00:30:00"
    extra: "--constraint=fast_io"

  - name: compute_scheduler
    account: compute_project
    gres: null
    mem: 16G
    nodes: 1
    ntasks_per_node: 1
    partition: compute
    qos: normal
    tmp: 10G
    walltime: "01:00:00"
    extra: "--constraint=haswell"

  - name: memory_scheduler
    account: memory_project
    gres: null
    mem: 64G
    nodes: 1
    ntasks_per_node: 1
    partition: highmem
    qos: normal
    tmp: 50G
    walltime: "02:00:00"
    extra: "--constraint=memory_optimized"

  - name: gpu_scheduler
    account: gpu_project
    gres: "gpu:v100:1"
    mem: 32G
    nodes: 1
    ntasks_per_node: 1
    partition: gpu
    qos: high
    tmp: 100G
    walltime: "04:00:00"
    extra: "--constraint=v100"